{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Home assignment 9\n",
    "\n",
    "(deadline 24.01.2021 22:00 MSK)\n",
    "\n",
    "You should prepare solutions of the presented problems in this Jupyter Notebook and submit it in the following [form](https://forms.gle/fLgiGmmk2gEM9fge9) \n",
    "\n",
    "Please, rename the Jupyter Notebook that you will submit as ```Surname_assignment9.ipynb```, where instead of ```Surname``` you write your family name. A solution of every problem should be placed below of the corresponding problem statement.\n",
    "\n",
    "After the running commands (Kernel -> Restart & Run All) all cells in your file have to run correctly. Please check this before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (4 pts)\n",
    "\n",
    "- (2 pts) Consider the problem\n",
    "\n",
    "$$\n",
    "\\log (e^x + e^{-x}) \\to \\min,\n",
    "$$\n",
    "\n",
    "implement and run Newton method with constant step size  $\\alpha = 1$ and initial guesses $x_0 = 1$ and $x_0 = 1.1$. What do you observe and why?\n",
    "- (1 pts) What is changed if you use damped Newton method? Why?\n",
    "- (1 pts) Make convergence plots for every experiment setting and compute the running time of every method. Do not forget to add legends and axis labels. As a convergence mrasure, use the norm of gradient in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (8 pts)\n",
    "\n",
    "Consider the optimization problem from the problem 3 in assignment 6 again. \n",
    "- (5 pts) Solve this problem with BFGS and L-BFGS methods from [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) package. Compare the running time and make a convergence plot with notm of gradient in every iteration. \n",
    "- (3 pts) Compare numerically how the size of history in L-BFGS method affects the running time and the number of iterations for convergence to desired accuracy in terms of gradient norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (4 pts)\n",
    "In this problem you need to prove Sherman-Morrison-Woodbury formula  \n",
    "\n",
    "- (2 pts) Given matrices $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$ and $I + AB$ is invertible. Show that $I + BA$ is invertible and $(I + BA)^{-1}B = B(I + AB)^{-1}$ \n",
    "- (0.5 pts) Derive the chain of equalities $(I+P)^{-1} = I - (I+P)^{-1}P = I - P(I+P)^{-1}$\n",
    "- (0.5 pts) Combine claims from previous subtasks and show that $(I + UV)^{-1} = I - UV(I + UV)^{-1} = I - U(I + VU)^{-1}V$\n",
    "- (1 pts) Generalize the result of previous subtask for the matrix $A + UV$ and derive the Sherman-Morrison-Woodbury formula \n",
    "\n",
    "$$\n",
    "(A + UV)^{-1} = A^{-1} - A^{-1} U \\left( I_k + V A^{-1} U \\right)^{-1} V A^{-1},\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{n \\times n}$, $U \\in \\mathbb{R}^{n \\times k}$ and $V \\in \\mathbb{R}^{k \\times n}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (10 pts)\n",
    "\n",
    "Consider the following problem\n",
    "$$\n",
    "\\min_{\\mathbf{x}} \\; \\sum_{i=1}^n f_i(x_i) + f_0(\\mathbf{A}\\mathbf{x} + \\mathbf{b}),\n",
    "$$ \n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{p \\times n}$ and $p \\ll n$, all functions are convex and sufficiently smooth.\n",
    "\n",
    "- (2 pts) Derive the formula to compute the hessian of this function\n",
    "- (4 pts) How the form of the hessian can be used to speed up computation of the direction in Newton method? Estimate complexity of this procedure and compare it with the general case\n",
    "- (4 pts) Generate the random instance of this problem and numerically demonstrate how the hessian structure speeds up Newton method compare with the general purpose procedure. Use the following functions:\n",
    "    - $f_0(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x}\\|_2^2$ \n",
    "    - $f_i(x_i) = \\frac{1}{2}(x_i - y_i)^2$, where $y_i$ is an element of the given vector $\\mathbf{y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (17 pts)\n",
    "\n",
    "Consider binary least squares problem\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min_x \\|Ax - b\\|_2^2 \\\\\n",
    "\\text{s.t. } & x_i^2 = 1,\n",
    "\\end{align*}\n",
    "\n",
    "where $A \\in \\mathbb{R}^{m \\times n}$, $m > n$ is a full-rank matrix. \n",
    "\n",
    "Application of this problem: assume we have a signal $x_i \\in \\{+1, -1 \\}^n$ and transmit it over the noisy channel such that the received signal is $b = Ax + v$, where $v \\sim \\mathcal{N}(0, \\sigma^2 I)$ is a random normal vector. So, the solution of our problemn is maximum likelihood estimation of the initial signal $x$.\n",
    "\n",
    "- (1 pts) Generate the random instance of this problem, i.e. matrix $A$, vector $b$ and the ground-truth solution $x$ for $n \\sim 10$ and $m \\sim 10^2$\n",
    "- (6 pts) Solve the original problem with the [Nevergrad](https://github.com/facebookresearch/nevergrad) library. Compare convergence of particle swarm optimization algorithm, genetic algorithm and OnePlusOne algorithm. Plot the convergence in terms of difference (think how to measure the diffetrence in this problem) between ground-truth solution and current approximation\n",
    "- (4 pts) Derive the convex relaxation of the original problem\n",
    "- (2 pts) Solve the obtained problem with CVXPy\n",
    "- (4 pts) Compare different ways to recover the approximate solution of the original problem from the solution of the relaxed convex problem:\n",
    "    - Compute the sign of the optimal vector in the relaxed convex problem\n",
    "    - Compute the leading eigenvector of the optimal matrix in the relaxed convex problem and take the elementwise sign of this vector \n",
    "    - Compute the solution of the unconstrained least-squares problem $\\|Ax - b\\|_2^2 \\to \\min_x$ and take the sign of the result\n",
    "\n",
    "Make a conclusion what approach gives the best approximation to the ground-truth solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
