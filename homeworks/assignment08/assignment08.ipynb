{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Home assignment 8\n",
    "\n",
    "(deadline 16.12.2020 20:00 MSK)\n",
    "\n",
    "You should prepare solutions of the presented problems in this Jupyter Notebook and submit it in the following [form](https://forms.gle/t1PJo2kY5DqpfxrDA) \n",
    "\n",
    "Please, rename the Jupyter Notebook that you will submit as ```Surname_assignment8.ipynb```, where instead of ```Surname``` you write your family name. A solution of every problem should be placed below of the corresponding problem statement.\n",
    "\n",
    "After the running commands (Kernel -> Restart & Run All) all cells in your file have to run correctly. Please check this before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (11 pts)\n",
    "\n",
    "Consider the problem \n",
    "\n",
    "$$ \\min_x \\frac{1}{2} \\| A x - b \\|_2^2, $$\n",
    "\n",
    "where matrix $A \\in \\mathbb{R}^{m \\times n}$ ($m > n$) and vector $b$ are given.\n",
    "\n",
    "- (1 pts) Derive the mathematical expression of stochastic gradient of this objective function for different size of batch \n",
    "- (5 pts) Implement SAG algorithm\n",
    "- (5 pts) Compare the convergence of SAG, SGD and ADAM for this simple problem for $m \\sim 10^3$ and $n \\sim 10^2$ and random data. Test different step size selection strategies (at least two) in SAG and SGD. Plot convergence plots for every experiment and make a conclusion about the performance of the studied methods in this particular problem.\n",
    "\n",
    "In this problem you can use automatic differentiation framework (JAX or PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (15 pts)\n",
    "\n",
    "Consider the optimization problem from the problem 3 in assignment 6. \n",
    "- (10 pts) Compare ADAM, SGD and accelerated gradient method that uses stochastic gradient estimation on this problem and discuss what method works better and why? \n",
    "- (5 pts) Illustrate your conclusion by convergence plots and test different strategies of step size selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
