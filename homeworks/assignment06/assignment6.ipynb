{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Home assignment 6\n",
    "\n",
    "(deadline 30.11.2020 10:00 MSK)\n",
    "\n",
    "You should prepare solutions of the presented problems in this Jupyter Notebook and submit it in the following [form](https://forms.gle/GEk9BNR9b3h2zhbK9) \n",
    "\n",
    "Please, rename the Jupyter Notebook that you will submit as ```Surname_assignment6.ipynb```, where instead of ```Surname``` you write your family name. A solution of every problem should be placed below of the corresponding problem statement.\n",
    "\n",
    "After the running commands (Kernel -> Restart & Run All) all cells in your file have to run correctly. Please check this before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Problem 1 (10 pts)\n",
    "\n",
    "- (2 pts) Prove that gradients, computed in two sequential points, generated by the steepest gradient descent, are orthogonal\n",
    "\n",
    "- (5 pts) Prove that if the objective function is $f(x) = \\frac{1}{2}x^{\\top}Qx - b^{\\top}x$, $Q \\in \\mathbb{S}^n_{++}$, step size is chosen according to the steepest descent rule and $x^0 - x^*$ is parallel to the eigenvector of the matrix $Q$, then gradient descent converges after one iteration. \n",
    "\n",
    "- Show that for the function $f(x) = x^{\\top}x$ the steepest gradient descent converges after one iteration using\n",
    "    - (1 pts) previous claim\n",
    "    - (2 pts) direct computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 pts)\n",
    "\n",
    "- (6 pts) Solve the foloowing problem with the steepest gradient descent method\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}x^{\\top}Ax - b^{\\top}x \\to \\min_x\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0.78 & −0.02 & −0.12 & −0.14\\\\\n",
    "−0.02 & 0.86 & −0.04 & 0.06 \\\\\n",
    "−0.12 & −0.04 & 0.72 & −0.08\\\\\n",
    "−0.14 & 0.06 & −0.08 & 0.74\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "b = \\begin{bmatrix}\n",
    "0.76\\\\\n",
    "0.08\\\\\n",
    "1.12\\\\\n",
    "0.68\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and initial guess $x^0 = 0$ with tolerance of the gradient norm equal to $10^{-6}$. \n",
    "\n",
    "- (2 pts) Make convergence plot in terms of the decreasing of gradient norm and make a conclusion on the convergence speed \n",
    "- (2 pts) Find spectrum of the matrix $A$ with a proper ```numpy``` function and compare the derived convergence plot with theoretical upper bound. How is the theoretical upper bound tight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (16 pts)\n",
    "\n",
    "- (1 pts) Consider the simple binary classification problem of data taken with [this utility](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). So the first step in this problem is downlowding the dataset and derive matrix $X \\in \\mathbb{R}^{m \\times n}$ of given data and vector $y \\in \\mathbb{R}^m$ of labels. \n",
    "- Now to train the simple logistic regression model you need to solve the following optimization problem\n",
    "\n",
    "$$ -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log(h(w, b | x_i)) + (1 - y_i)\\log(1 - h(w, b | x_i))) \\to \\min_{w, b},$$\n",
    "\n",
    "where $y_i$ is an $i$-th label, $x_i$ is an $i$-th row of the matrix $X$, $h(w, b | x_i) = \\sigma(w^{\\top}x_i + b)$ (here we use $x_i$ as a column vector) and $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. This problem has two parameter $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. It is possible to incorporate them in the single vector, so if you want, you can consider them as a components of the single vector.\n",
    "    \n",
    "The explanation of the form of the objective function and related topics on how to measure the quality of the trained model will be discussed in the machine learning course.\n",
    "\n",
    "- (5 pts) Implement the gradient of this objective function and compare your implementation with automatically computed gradient in JAX. Loops are prohibited in your implementation!\n",
    "- (5 pts) Consider four constant step sizes $10^{-3}, 10^{-2}, 10^{-1}, 1$ and compare the convergence of the gradient descent for them. Also fix the initialization in every run. You should get three convergence plots and analyze them.\n",
    "- (3 pts) Compare the obtained results with the results if Armijo rule to adapt the step size is used. Provide convergence plots and running time in seconds in all experiments.\n",
    "-  (2 pts) Make a conclusion on the gain that can be established by the proper adaptive step size selection rules \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
