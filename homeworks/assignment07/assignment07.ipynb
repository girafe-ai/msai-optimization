{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Home assignment 7\n",
    "\n",
    "(deadline 10.12.2020 20:00 MSK)\n",
    "\n",
    "You should prepare solutions of the presented problems in this Jupyter Notebook and submit it in the following [form](https://forms.gle/HjzYBUatVcf72QHT7) \n",
    "\n",
    "Please, rename the Jupyter Notebook that you will submit as ```Surname_assignment7.ipynb```, where instead of ```Surname``` you write your family name. A solution of every problem should be placed below of the corresponding problem statement.\n",
    "\n",
    "After the running commands (Kernel -> Restart & Run All) all cells in your file have to run correctly. Please check this before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (1 pts)\n",
    "\n",
    "Prove that conjugate directions are linearly independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5 pts)\n",
    "\n",
    "Show that the direction for update $x_k$ in the conjugate gradient method for the quadratic strongly convex objective function is the descent direction. \n",
    "Is it the correct claim abpout the direction in the Fletcher-Reeves method? Explain why and discuss the role of the step size selection procedure in determining whether a direction is descent or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Problem 3 (8 pts)\n",
    "\n",
    "Consider the following optimization problem\n",
    "\n",
    "$$\n",
    "\\min_x \\log \\left( \\sum_{i=1}^m \\exp(a_i^{\\top}x + b_i)\\right).\n",
    "$$\n",
    "\n",
    "- (2 pts) Compare convergence of gradient descent, non-linear conjugate grdient method from ```scipy.optimize``` package, heavy ball method and accelerated gradient method in solving this problem if the gradient norm is used as a measure of convergence \n",
    "\n",
    "- (4 pts) What procedure to select the step size gives the fastest convergence for every method? Consider any three strategies for every of the following method: gradient descent, heavy ball method and accelerated gradient method. Examples are constant, decreasing sequence, some adaptive rule. In the latter case, think how you can adapt the Armijo rule (or similar rules) for heavy ball and accelerated gradient method.\n",
    "- (2 pts) Consider different dimensions $m$ and $n$ ($m > n$ and $m < n$). Vectors $a_i$ and $b$ canbe genrated fromthe standard normal distribution.\n",
    "\n",
    "Note that the stable way to compute the objective and gradient is crucial to solve this problem successfully, otherwise you will get NaN values in vector $x_k$ since the intermediate values for the sum of large exponent overflow the capacity of the ```double``` precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
